\documentclass[a4paper]{article}
\usepackage[darkBlue]{ahsansabit}	

\title{Honors Linear Algebra: Class 02} 
\date{11 January, 2024}
\author{Ahmed Saad Sabit, Rice University}

\begin{document}
\maketitle

\df{
A \emph{Vector Space} is a set $V$ along with an addition on $V$ and a scalar multiplication on $V$ such that the following properties hold
\begin{itemize}
	\item \textbf{commutativity}: $u + v = v + u$ if $u,v \in V$.
	\item \textbf{associativity}: $(u + v) + w= u + (v + w) $ and $(ab)v = a (bv)$ for all $u,v,w \in V$ and for all $a,b \in \mathbb{F}$. 
	\item \textbf{additive identity}: There exists an element $0 \in V$ such that $v+0 = v$ for all $v \in V$. 
        \item \textbf{additive inverse}: For every $v \in V$, there exists $w \in V$ such that $v + w = 0$. 
	\item \textbf{multiplicative identity}: $1 v = v$ for all $v \in V$.   
        \item \textbf{distributive properties}: $a(v+u)=au+av$ and $(a+b)v = av + bv$ for all $a,b \in \mathbb{F}$ and all $u,v \in V$.
\end{itemize}
}

\df{
	A set $U$ is the subspace of $V$ if it includes additive identity $\{0\}$, and has a closure. Which can be said,
	\begin{itemize}
		\item $0 \in U$ 
		\item $u , w \in U$ then $u + w \in U$
		\item $a \in \mathbb{F}$ and $u \in U$, then $au \in U$
	\end{itemize}
}
A good example is to look at $\mathbb{R}^{3}$, let's consider \textbf{all the vectors} that have the form,
\[
\vec{v} = \begin{pmatrix} a \\ 0 \\ -a \end{pmatrix} 
\] where $a \in \mathbb{R}$. Does the all possible $\vec{v}$ build a subspace? 

Let's consider each of the cases. It must have a $0$ and it must follow closure. First condition is easy to see, for $a = 0$, we have $\vec{v} = \left(0,0,0\right)$ which is the zero. Now, for closure, let's take two vectors $\vec{v}$ and $\vec{p}$ that has the form $(a,0,-a)$. 

\[
\vec{v} + \vec{p} = \begin{pmatrix} a \\ 0 \\ -a\end{pmatrix}  
+ 
\begin{pmatrix} b \\ 0 \\ -b \end{pmatrix} = 
\begin{pmatrix} a + b \\ 0 \\ - \left(a + b\right) \end{pmatrix} 
\] 
We wanted the vectors to have the form of $(a,0,-a)$, and the addition of two random vectors turns out to have the same exact form $(a+b, 0, -(a+b))$. Second condition seems to be satisfied. 

Now the third condition is pretty easy to see, if we multiply any number $c$ (which can be complex), then, 
\[
c \vec{v} = c \begin{pmatrix} a \\ 0 \\ -a \end{pmatrix}  = 
\begin{pmatrix} ca \\ 0 \\ -ca \end{pmatrix} 
\] 
Still the same form, third condition is solved, we have a successful vector space!

To recap, our subspace is set of all vectors such that, 
\[
U = \{ 
	(x, 0, -x) : x \in \mathbb{F}
\}
\]
The vector space is by the way,
\[
\mathbb{R}^3 = V = \{
	(x,y,z) : x,y,z \in \mathbb{F}
\}
\]
\begin{figure}[ht]
    \centering
    \incfig{test-diagram-for-linear-algebra}
    \caption{Test diagram for Linear Algebra}
    \label{fig:test-diagram-for-linear-algebra}
\end{figure}


\section{Principle of Mathematical Induction}
Let $P(a)$ be a statement indexed by positive integers $n \in N$. Suppose, $P(1)$ is true. This is called the Base Case.

\begin{itemize}
	\item Suppose $P(1)$ is true. (Base)
	\item If $P(k)$ is true, then $P(k+1)$ is true. (Inductive step)
\end{itemize}
 
Then $P(n)$ is true for all $n \in N$. You can prove infinitely many statements to be true. But you don't need to prove each one by one. So you try to prove at least one of them, and then you can prove that if $P(k )$ is true, then $P(1+k)$ is true. 

\pr{Prove.
	\[P(n): 0+1+2+\ldots+n = \frac{n (n+1)}{2}\] for all $n \ge 0$.
}
\solu{
Base case $P(0) = 0$ is true. 
Suppose $P(k)$ is true. Then,
\[
P(k+1) = 0 + 1 + \ldots + k + (k+1) = P(k) + (k+1) = \frac{k (k+1)}{2} + (k+1) = \frac{(k+1)(k+2)}{2}
\]
As we assumed $P(k)$ as true, then for $P(k+1)$, we have to add that to $P(k)$ formula. Because it held up for the next one so this statement is proved.
}

\pr{
Prove. 
\[
1^2 + 2^2 + \cdots + n^2 = \frac{n (n+1)(2n + 1)}{6}
\] 
}
\solu{
Let's get the base case. 
\[
P(1) = 1
\] It is true. 

Let's see $P(n)$ and assume that it is true. (Inductive assumption) Let's try $P(n+1)$, which is,
\[
P(n+1) = P(n) + (n+1)^2 = \frac{n(n+1)(2n+1)}{6} + (n+1)^2
\] 
Ah! Algebra only! If you do it, you just need to show that equals
\[
P(n+1) = \frac{(n+1)((n+1)+1)(2(n+1)+1)}{6}
\] 
}

\section{Sum of Subspaces (Axler. 1C)}
$V$ is a vector space. And we have subspaces $U_1, U_2, U_3, \cdots, U_n$. Now,
\[
	\sum_{n=1}^{n_0} U_n = \{ v_1 + v_2 + \cdots + v_n | v_k \in U_k \}
\]
It's the sum of all possible vectors the subspaces offer.
\begin{figure}[ht]
    \centering
    \incfig{addition-of-subspaces}
    \caption{Addition of Subspaces}
    \label{fig:addition-of-subspaces}
\end{figure}

\section{Direct Sums $\oplus$}
\df{Assume  $U_1, U_2, U_3, \ldots$ are subspaces of $V$. Then the subspace $\sum_{n}^{n_0} U_n$ is called a direct sum if every $v \in \sum U_n$ equals, 
\[
v = u_1 + u_2 + \cdots + u_n
\] Then we say that is a direct sum. The notation, 
\[
v = u_1 \oplus u_2 \oplus \cdots
\] }

Let's say that we have three subspaces,
\[
	U_1 = \{(t,0,0)\}
\] 
\[
	U_2 = \{(0,t,0)\}
\] 
\[
	U_3 = \{(0,0,t)\}
\]
Now for $x \in \mathbb{R}^3$, 
\[
x = (x_1, 0,0) + (0,x_2,0) + (0,0,x_3)
\] 
This is a direct sum,
\[
\mathbb{R}^3 = U_1 \oplus U_2 \oplus U_3
\] 

Another example, 
\[
U_1 = (t,s,0)
\] 
\[
U_2 = (t,0,s)
\] 
Now if we define $\mathbb{R}^3$ where $x = (x_1,x_2,x_3) = c_1 (t,s,0) + c_2 (t', 0, s')$, then,
\begin{align*}
	x_1 &= c_1 t + c_2 t' \\
	x_2 &= c_1s \\
	x_3 &=  c_2 s'
.\end{align*}
This is a sum where $\mathbb{R}^3$ is a sum but not direct sum $U_1 + U_2$.

\theo{
Condition for a direct sum. 
\[
V = U_1 + \cdots + U_n
\] 
This sum is direct if and only if there is only one way to write the origin $\{0\}$
 \[
	 \{0\} = v_1 + \cdots + v_n
\] 
Where $v_k \in U_k$ implies $v_1 = v_2 = v_n = 0$.
}


\pf{
Every vector in $V$ can be written in a unique way as a sum of vectors from $U_1$ to $U_n$. In particular the origin can be written in a unique way, one way is to use the $0$, but because this is a direct sum, that is the only way to do that.

But say a vector can be written in two ways in the sum (it's not direct sum). So, 
\[
\vec{v} = \sum \vec{u}_n = \sum \vec{u}_n'
\]
If this is a direct sum then only possible way to do this is when $\vec{u}_n = \vec{u}_n'$, and then the origin can be written as,
\[
\vec{0} = \sum (\vec{u}_n - \vec{u}_n') = 0
\] 
}

\section{Ch2 Review}
\df{
$v_1, \ldots, v_2$ in $V$ is linearly independent if, 
\[
a_1 v_1 + \ldots + a_n v_n = 0
\] 
then $a_1 = \ldots = a_n = 0$. They are linearly dependent if and only if they don't satisfy above condition (linear independence).
}
So if you can pull up a sum like that without all the $a_n$ being 0 then you just got yourself a linearly dependent system.

\theo{The linear dependence Lemma (TODO)
Suppose $v_1, v_2, \ldots, v_n$ in $V$ are linearly dependent, then $\exists $ an equation,
$a_1 v_1 + \ldots + a_n v_n$ not all $a_k = 0$. 
}
Let's choose the last possible $a_k$ so that it is not zero. 
\[
	a_j v_j = - a_1 v_1 - \ldots - a_{j-1} v_{j-1}
\]
\[
v_j = - \frac{a_1}{a_j} v_1 - \ldots - \frac{a_{j-1}}{a_j} v_{j-1}
\] 
So there exists $j$ such that $v_j$ is a linear combination of the others. Consider, 
\[
	\text{span}(v_1, v_2, v_3, \ldots,v_j)
\] 
But $v_j$ is the combination of the others already, so, 
\[
	= \text{span}(v_1, v_2, \ldots , v_k)
\]
So the span didn't change though I threw away the term. We can more efficiently calculate the span. 

Lemma: Anything you prove in the middle of proving something bigger.

\[
\sum_{n=1}^{\infty} a_n z^n
\]

\begin{figure}[ht]
    \centering
    \incfig{diagram}
    \caption{diagram}
    \label{fig:diagram}
\end{figure}
\end{document}
